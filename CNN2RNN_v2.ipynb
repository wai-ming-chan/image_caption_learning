{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde443b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-29 20:17:57.790426: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages/spacy/util.py:837: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.3.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 6, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "import os  # when loading file paths\n",
    "import pandas as pd  # for lookup in annotation file\n",
    "import spacy  # for tokenizer\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence  # pad batch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image  # Load img\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# We want to convert text -> numerical values\n",
    "# 1. We need a Vocabulary mapping each word to a index\n",
    "# 2. We need to setup a Pytorch dataset to load the data\n",
    "# 3. Setup padding of every batch (all examples should be\n",
    "#    of same seq_len and setup dataloader)\n",
    "# Note that loading the image is very easy compared to the text!\n",
    "\n",
    "# Download with: python -m spacy download en\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\") # spacy.load(\"en\")\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    def tokenizer_eng(text):\n",
    "\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "\n",
    "        frequencies = {}\n",
    "\n",
    "        idx = 4\n",
    "\n",
    "\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "\n",
    "                if word not in frequencies:\n",
    "\n",
    "                    frequencies[word] = 1\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "\n",
    "                    frequencies[word] += 1\n",
    "\n",
    "\n",
    "\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "\n",
    "                    self.stoi[word] = idx\n",
    "\n",
    "                    self.itos[idx] = word\n",
    "\n",
    "                    idx += 1\n",
    "\n",
    "\n",
    "\n",
    "    def numericalize(self, text):\n",
    "\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "\n",
    "\n",
    "        return [\n",
    "\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "\n",
    "            for token in tokenized_text\n",
    "\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FlickrDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        self.df = pd.read_csv(captions_file)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "\n",
    "        # Get img, caption columns\n",
    "\n",
    "        self.imgs = self.df[\"image\"]\n",
    "\n",
    "        self.captions = self.df[\"caption\"]\n",
    "\n",
    "\n",
    "\n",
    "        # Initialize vocabulary and build vocab\n",
    "\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        caption = self.captions[index]\n",
    "\n",
    "        img_id = self.imgs[index]\n",
    "\n",
    "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
    "\n",
    "\n",
    "\n",
    "        if self.transform is not None:\n",
    "\n",
    "            img = self.transform(img)\n",
    "\n",
    "\n",
    "\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "\n",
    "\n",
    "        return img, torch.tensor(numericalized_caption)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyCollate:\n",
    "\n",
    "    def __init__(self, pad_idx):\n",
    "\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "\n",
    "\n",
    "    def __call__(self, batch):\n",
    "\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "\n",
    "        targets = [item[1] for item in batch]\n",
    "\n",
    "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
    "\n",
    "\n",
    "\n",
    "        return imgs, targets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_loader(\n",
    "\n",
    "    root_folder,\n",
    "\n",
    "    annotation_file,\n",
    "\n",
    "    transform,\n",
    "\n",
    "    batch_size=32,\n",
    "\n",
    "    num_workers=8,\n",
    "\n",
    "    shuffle=True,\n",
    "\n",
    "    pin_memory=True,\n",
    "\n",
    "):\n",
    "\n",
    "    dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "\n",
    "\n",
    "    loader = DataLoader(\n",
    "\n",
    "        dataset=dataset,\n",
    "\n",
    "        batch_size=batch_size,\n",
    "\n",
    "        num_workers=num_workers,\n",
    "\n",
    "        shuffle=shuffle,\n",
    "\n",
    "        pin_memory=pin_memory,\n",
    "\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx),\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    return loader, dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "\n",
    "    [transforms.Resize((224, 224)), transforms.ToTensor(),]\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "loader, dataset = get_loader(\n",
    "\n",
    "    \"data/flickr8k/images/\", \"data/flickr8k/captions.txt\", transform=transform\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "if False:\n",
    "    for idx, (imgs, captions) in enumerate(loader):\n",
    "        print(imgs.shape)\n",
    "        print(captions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c50c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def print_examples(model, device, dataset):\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "\n",
    "        [\n",
    "\n",
    "            transforms.Resize((299, 299)),\n",
    "\n",
    "            transforms.ToTensor(),\n",
    "\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "\n",
    "        ]\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_img1 = transform(Image.open(\"test_examples/dog.jpg\").convert(\"RGB\")).unsqueeze(\n",
    "\n",
    "        0\n",
    "\n",
    "    )\n",
    "\n",
    "    print(\"Example 1 CORRECT: Dog on a beach by the ocean\")\n",
    "\n",
    "    print(\n",
    "\n",
    "        \"Example 1 OUTPUT: \"\n",
    "\n",
    "        + \" \".join(model.caption_image(test_img1.to(device), dataset.vocab))\n",
    "\n",
    "    )\n",
    "\n",
    "    test_img2 = transform(\n",
    "\n",
    "        Image.open(\"test_examples/child.jpg\").convert(\"RGB\")\n",
    "\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    print(\"Example 2 CORRECT: Child holding red frisbee outdoors\")\n",
    "\n",
    "    print(\n",
    "\n",
    "        \"Example 2 OUTPUT: \"\n",
    "\n",
    "        + \" \".join(model.caption_image(test_img2.to(device), dataset.vocab))\n",
    "\n",
    "    )\n",
    "\n",
    "    test_img3 = transform(Image.open(\"test_examples/bus.png\").convert(\"RGB\")).unsqueeze(\n",
    "\n",
    "        0\n",
    "\n",
    "    )\n",
    "\n",
    "    print(\"Example 3 CORRECT: Bus driving by parked cars\")\n",
    "\n",
    "    print(\n",
    "\n",
    "        \"Example 3 OUTPUT: \"\n",
    "\n",
    "        + \" \".join(model.caption_image(test_img3.to(device), dataset.vocab))\n",
    "\n",
    "    )\n",
    "\n",
    "    test_img4 = transform(\n",
    "\n",
    "        Image.open(\"test_examples/boat.png\").convert(\"RGB\")\n",
    "\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    print(\"Example 4 CORRECT: A small boat in the ocean\")\n",
    "\n",
    "    print(\n",
    "\n",
    "        \"Example 4 OUTPUT: \"\n",
    "\n",
    "        + \" \".join(model.caption_image(test_img4.to(device), dataset.vocab))\n",
    "\n",
    "    )\n",
    "\n",
    "    test_img5 = transform(\n",
    "\n",
    "        Image.open(\"test_examples/horse.png\").convert(\"RGB\")\n",
    "\n",
    "    ).unsqueeze(0)\n",
    "\n",
    "    print(\"Example 5 CORRECT: A cowboy riding a horse in the desert\")\n",
    "\n",
    "    print(\n",
    "\n",
    "        \"Example 5 OUTPUT: \"\n",
    "\n",
    "        + \" \".join(model.caption_image(test_img5.to(device), dataset.vocab))\n",
    "\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
    "\n",
    "    print(\"=> Saving checkpoint\")\n",
    "\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "\n",
    "    print(\"=> Loading checkpoint\")\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    step = checkpoint[\"step\"]\n",
    "\n",
    "    return step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a802ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import statistics\n",
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, train_CNN=False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        self.inception = models.inception_v3(pretrained=True, aux_logits=True)\n",
    "#         self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.times = []\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.inception(images)\n",
    "        return self.dropout(self.relu(features))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "\n",
    "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
    "\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "\n",
    "        outputs = self.linear(hiddens)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CNNtoRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "\n",
    "        self.encoderCNN = EncoderCNN(embed_size)\n",
    "\n",
    "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "\n",
    "        features = self.encoderCNN(images)\n",
    "\n",
    "        outputs = self.decoderRNN(features, captions)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "    def caption_image(self, image, vocabulary, max_length=50):\n",
    "\n",
    "        result_caption = []\n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            x = self.encoderCNN(image).unsqueeze(0)\n",
    "\n",
    "            states = None\n",
    "\n",
    "\n",
    "\n",
    "            for _ in range(max_length):\n",
    "\n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
    "\n",
    "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
    "\n",
    "                predicted = output.argmax(1)\n",
    "\n",
    "                result_caption.append(predicted.item())\n",
    "\n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "\n",
    "                    break\n",
    "\n",
    "\n",
    "\n",
    "        return [vocabulary.itos[idx] for idx in result_caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0be9be88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "                                        \r"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "relu(): argument 'input' (position 1) must be Tensor, not InceptionOutputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 193\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m----------------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    187\u001b[0m         print_examples(model, device, dataset)\n\u001b[0;32m--> 193\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 161\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    157\u001b[0m captions \u001b[38;5;241m=\u001b[39m captions\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 161\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m    164\u001b[0m \n\u001b[1;32m    165\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]), captions\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m )\n\u001b[1;32m    171\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem(), global_step\u001b[38;5;241m=\u001b[39mstep)\n",
      "File \u001b[0;32m/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 79\u001b[0m, in \u001b[0;36mCNNtoRNN.forward\u001b[0;34m(self, images, captions)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, captions):\n\u001b[0;32m---> 79\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoderCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoderRNN(features, captions)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mEncoderCNN.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images):\n\u001b[1;32m     26\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minception(images)\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/activation.py:102\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTypeError\u001b[0m: relu(): argument 'input' (position 1) must be Tensor, not InceptionOutputs"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# from utils import save_checkpoint, load_checkpoint, print_examples\n",
    "# from get_loader import get_loader\n",
    "# from model import CNNtoRNN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "\n",
    "        [\n",
    "\n",
    "            transforms.Resize((356, 356)),\n",
    "\n",
    "            transforms.RandomCrop((299, 299)),\n",
    "\n",
    "            transforms.ToTensor(),\n",
    "\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "\n",
    "        ]\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    train_loader, dataset = get_loader(\n",
    "\n",
    "        root_folder=\"data/flickr8k/images\",\n",
    "\n",
    "        annotation_file=\"data/flickr8k/captions.txt\",\n",
    "\n",
    "        transform=transform,\n",
    "\n",
    "        num_workers=2,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    load_model = False\n",
    "\n",
    "    save_model = False\n",
    "\n",
    "    train_CNN = False\n",
    "\n",
    "\n",
    "\n",
    "    # Hyperparameters\n",
    "\n",
    "    embed_size = 256\n",
    "\n",
    "    hidden_size = 256\n",
    "\n",
    "    vocab_size = len(dataset.vocab)\n",
    "\n",
    "    num_layers = 1\n",
    "\n",
    "    learning_rate = 3e-4\n",
    "\n",
    "    num_epochs = 1\n",
    "\n",
    "\n",
    "\n",
    "    # for tensorboard\n",
    "\n",
    "    writer = SummaryWriter(\"runs_flickr\")\n",
    "\n",
    "    step = 0\n",
    "\n",
    "\n",
    "\n",
    "    # initialize model, loss etc\n",
    "\n",
    "    model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "    # Only finetune the CNN\n",
    "\n",
    "    for name, param in model.encoderCNN.inception.named_parameters():\n",
    "\n",
    "        if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "\n",
    "            param.requires_grad = True\n",
    "\n",
    "        else:\n",
    "\n",
    "            param.requires_grad = train_CNN\n",
    "\n",
    "\n",
    "\n",
    "    if load_model:\n",
    "\n",
    "        step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "\n",
    "\n",
    "\n",
    "    model.train()\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Uncomment the line below to see a couple of test cases\n",
    "\n",
    "        # print_examples(model, device, dataset)\n",
    "\n",
    "\n",
    "\n",
    "        if save_model:\n",
    "\n",
    "            checkpoint = {\n",
    "\n",
    "                \"state_dict\": model.state_dict(),\n",
    "\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "\n",
    "                \"step\": step,\n",
    "\n",
    "            }\n",
    "\n",
    "            save_checkpoint(checkpoint)\n",
    "\n",
    "\n",
    "\n",
    "        for idx, (imgs, captions) in tqdm(\n",
    "            enumerate(train_loader), total=len(train_loader), leave=False\n",
    "        ):\n",
    "            imgs = imgs.to(device)\n",
    "            captions = captions.to(device)\n",
    "            outputs = model(imgs, captions[:-1])\n",
    "            loss = criterion(\n",
    "                outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1)\n",
    "            )\n",
    "\n",
    "            writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
    "            step += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(loss)\n",
    "            optimizer.step()\n",
    "\n",
    "        print('----------------------')\n",
    "\n",
    "        print_examples(model, device, dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1fe98d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
