{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-lzxhm0k1d8"
   },
   "source": [
    "Preprocessing and formating data to be used for model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bNmQcrIZkse6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /panfs/panfs.ittc.ku.edu/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages (4.27.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /panfs/panfs.ittc.ku.edu/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /panfs/panfs.ittc.ku.edu/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: requests in /panfs/panfs.ittc.ku.edu/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /panfs/panfs.ittc.ku.edu/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /panfs/panfs.ittc.ku.edu/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /panfs/panfs.ittc.ku.edu/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: filelock in /panfs/panfs.ittc.ku.edu/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages (from transformers) (3.10.7)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /panfs/panfs.ittc.ku.edu/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /panfs/panfs.ittc.ku.edu/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /panfs/panfs.ittc.ku.edu/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /panfs/panfs.ittc.ku.edu/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /panfs/panfs.ittc.ku.edu/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /panfs/panfs.ittc.ku.edu/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /panfs/panfs.ittc.ku.edu/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "import pandas as pd  \n",
    "import spacy  \n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image \n",
    "import torchvision.transforms as transforms\n",
    "import re\n",
    "! pip install transformers\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"} # integer to string \n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3} #STRING TO INTEGER\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        frequencies = {}\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            #tokenized_sentence = [tok.text.lower() for tok in spacy_eng.tokenizer(sentence)]\n",
    "            tokenized_sentence = tokenizer.tokenize(sentence)\n",
    "            for word in tokenized_sentence:\n",
    "                frequencies[word] = frequencies.get(word, 0) + 1\n",
    "\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "        return [self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "                for token in tokenized_text]\n",
    "    \n",
    "    \n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(captions_file)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.imgs = self.df[\"image\"]\n",
    "        self.captions = self.df[\"caption\"]\n",
    "\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return img, torch.tensor(numericalized_caption)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    \n",
    "        transforms.Resize((356, 356)),\n",
    "        transforms.RandomCrop((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "dataset = FlickrDataset(root_dir='data/flickr8k/images', \n",
    "                        captions_file='data/flickr8k/captions.txt', \n",
    "                        transform=transform,\n",
    "                        freq_threshold=5)\n",
    "\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        images = [item[0].unsqueeze(0) for item in batch]\n",
    "        images = torch.cat(images, dim=0)\n",
    "        \n",
    "        # Pad the sequences with zeros to make them the same length\n",
    "        lengths = [len(sample[1]) for sample in batch]\n",
    "        max_length = max(lengths)\n",
    "        padded_batch = torch.full((len(batch), max_length), self.pad_idx, dtype=torch.long)\n",
    "        for i, sample in enumerate(batch):\n",
    "            padded_batch[i, :len(sample[1])] = torch.LongTensor(sample[1])        \n",
    "\n",
    "        targets = padded_batch\n",
    "        \n",
    "        return images, targets    \n",
    "    \n",
    "    \n",
    "pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "\n",
    "loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        collate_fn=MyCollate(pad_idx=pad_idx))\n",
    "\n",
    "\n",
    "# x, y = next(iter(loader))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch\n",
    "\n",
    "# new_model = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "# new_model.fc\n",
    "\n",
    "class encoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, should_train=False):\n",
    "        super(encoderCNN, self).__init__()\n",
    "        self.should_train = should_train\n",
    "        self.inception = models.inception_v3(pretrained=True, aux_logits=True)\n",
    "#         self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
    "        self.dropout= nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.inception(x)\n",
    "        for name, param in self.inception.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "#         for name, param in self.inception.named_parameters():\n",
    "#             if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "#                 param.requires_grad = True\n",
    "#             else:\n",
    "#                 param.required_grad = self.should_train\n",
    "        return self.dropout(self.relu(features))\n",
    "    \n",
    "class decoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size,vocab_size, hidden_size, num_layers):\n",
    "        super(decoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, features, caption):\n",
    "        embeddings = self.dropout(self.embedding(caption))\n",
    "        embeddings = torch.cat((features.unsqueeze(0),embeddings), dim=0)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n",
    "    \n",
    "class CNN2RNN(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size, hidden_size, num_layers):\n",
    "        super(CNN2RNN, self).__init__()\n",
    "        self.encoderCNN = encoderCNN(embed_size)\n",
    "        self.decoderRNN = decoderRNN(embed_size, vocab_size, hidden_size, num_layers)    \n",
    "\n",
    "    def forward(self, images, caption):\n",
    "        x = self.encoderCNN(images)\n",
    "        x = self.decoderRNN(x, caption)\n",
    "        return x\n",
    "\n",
    "    def captionImage(self, image, vocabulary, maxlength=50):\n",
    "        result_caption = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(image).unsqueeze(0)\n",
    "            states = None\n",
    "\n",
    "            for _ in range(maxlength):\n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "                print(predicted.shape)\n",
    "                result_caption.append(predicted.item())\n",
    "                x = self.decoderRNN.embedding(output).unsqueeze(0)\n",
    "                \n",
    "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
    "                    break\n",
    "\n",
    "        return [vocabulary.itos[i] for i in result_caption]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /users/w880c134/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406f1194786347a485ed8f65e0eb5438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/104M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "relu(): argument 'input' (position 1) must be Tensor, not InceptionOutputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m         imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     64\u001b[0m         captions \u001b[38;5;241m=\u001b[39m captions\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 65\u001b[0m         score \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#         print(score.shape, captions.shape)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#         print(score.reshape(-1, score.shape[2]).shape, captions.reshape(-1).shape)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m#         print(\"why are we reshaping it here?\")\u001b[39;00m\n\u001b[1;32m     70\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[13], line 54\u001b[0m, in \u001b[0;36mCNN2RNN.forward\u001b[0;34m(self, images, caption)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, caption):\n\u001b[0;32m---> 54\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoderCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoderRNN(x, caption)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[13], line 30\u001b[0m, in \u001b[0;36mencoderCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m             param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#         for name, param in self.inception.named_parameters():\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#             if \"fc.weight\" in name or \"fc.bias\" in name:\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#                 param.requires_grad = True\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#             else:\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#                 param.required_grad = self.should_train\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/modules/activation.py:102\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/w880c134/anaconda3/envs/tf/lib/python3.9/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mTypeError\u001b[0m: relu(): argument 'input' (position 1) must be Tensor, not InceptionOutputs"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename = \"my_checkpoint.pth.tar\"):\n",
    "    print(\"saving checkpoint!\")\n",
    "    torch.save(state, filename)\n",
    "    \n",
    "def load_checkpoint(checkpoint, model, optimizer):\n",
    "    print(\"loading checkpoint!\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    model.load_state_optimizer(checkpoint[\"optimizer\"])\n",
    "    step = checkpoint[\"step\"]\n",
    "    return step\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "load_model = False\n",
    "save_model=False\n",
    "train_CNN = False\n",
    "\n",
    "# Hyperparameters\n",
    "import torch.optim as optim \n",
    "\n",
    "step = 0\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "num_layers = 5\n",
    "num_epochs = 5\n",
    "learning_rate = 3e-4\n",
    "vocab_size = len(dataset.vocab)\n",
    "\n",
    "model = CNN2RNN(\n",
    "    embed_size=embed_size, hidden_size=hidden_size,\n",
    "    vocab_size=vocab_size, num_layers=num_layers\n",
    ").to(device=device)\n",
    "model.decoderRNN\n",
    "\n",
    "loss_criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "if load_model:\n",
    "    step = load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model, optimizer)\n",
    "    \n",
    "    \n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if save_model:\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": model.state_dict(),\n",
    "            \"step\": step,\n",
    "        }\n",
    "        save_checkpoint(checkpoint)\n",
    "\n",
    "#     for idx, (imgs, captions) in tqdm(\n",
    "#         enumerate(loader), total=len(loader), leave=False\n",
    "#     ):\n",
    "\n",
    "    for idx, (imgs, captions) in enumerate(loader):\n",
    "        imgs = imgs.to(device)\n",
    "        captions = captions.to(device)\n",
    "        score = model(imgs, captions[:-1])\n",
    "\n",
    "#         print(score.shape, captions.shape)\n",
    "#         print(score.reshape(-1, score.shape[2]).shape, captions.reshape(-1).shape)\n",
    "#         print(\"why are we reshaping it here?\")\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_criterion(score.reshape(-1, score.shape[2]), captions.reshape(-1))\n",
    "        step += 1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Loss for epoch {epoch}: {loss}\")    \n",
    "    \n",
    "image_path = \"data/flickr8k/images/1032460886_4a598ed535.jpg\"\n",
    "img = Image.open(image_path)\n",
    "img = transform(img)\n",
    "plt.imshow(img.permute(1,2,0))\n",
    "image_input = img.to(device=device) # check here\n",
    "\n",
    "print(model.captionImage(image=image_input, vocabulary=dataset.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOQByr3wBwU94E9CBgMLoQt",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
