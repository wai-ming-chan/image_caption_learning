{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOQByr3wBwU94E9CBgMLoQt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Preprocessing and formating data to be used for model "],"metadata":{"id":"P-lzxhm0k1d8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bNmQcrIZkse6"},"outputs":[],"source":["import os  \n","import pandas as pd  \n","import spacy  \n","import torch\n","from torch.nn.utils.rnn import pad_sequence \n","from torch.utils.data import DataLoader, Dataset\n","from PIL import Image \n","import torchvision.transforms as transforms\n","import re\n","from transformers import BertTokenizer\n","\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","class Vocabulary:\n","    def __init__(self, freq_threshold):\n","        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"} # integer to string \n","        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3} #STRING TO INTEGER\n","        self.freq_threshold = freq_threshold\n","\n","    def __len__(self):\n","        return len(self.itos)\n","\n","    def build_vocabulary(self, sentence_list):\n","        frequencies = {}\n","        idx = 4\n","\n","        for sentence in sentence_list:\n","            #tokenized_sentence = [tok.text.lower() for tok in spacy_eng.tokenizer(sentence)]\n","            tokenized_sentence = tokenizer.tokenize(sentence)\n","            for word in tokenized_sentence:\n","                frequencies[word] = frequencies.get(word, 0) + 1\n","\n","                if frequencies[word] == self.freq_threshold:\n","                    self.stoi[word] = idx\n","                    self.itos[idx] = word\n","                    idx += 1\n","\n","    def numericalize(self, text):\n","        tokenized_text = tokenizer.tokenize(text)\n","\n","        return [self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n","                for token in tokenized_text]\n","    \n","    \n","class FlickrDataset(Dataset):\n","    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n","        self.root_dir = root_dir\n","        self.df = pd.read_csv(captions_file)\n","        self.transform = transform\n","\n","        self.imgs = self.df[\"image\"]\n","        self.captions = self.df[\"caption\"]\n","\n","        self.vocab = Vocabulary(freq_threshold)\n","        self.vocab.build_vocabulary(self.captions.tolist())\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        caption = self.captions[index]\n","        img_id = self.imgs[index]\n","        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n","        numericalized_caption += self.vocab.numericalize(caption)\n","        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n","\n","        return img, torch.tensor(numericalized_caption)\n","\n","\n","transform = transforms.Compose([\n","    \n","        transforms.Resize((356, 356)),\n","        transforms.RandomCrop((299, 299)),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","    ])\n","\n","dataset = FlickrDataset(root_dir='D:/pytorch_exercise/dataset/flickr8k/flickr8k/images', \n","                        captions_file='D:/pytorch_exercise/dataset/flickr8k/flickr8k/captions.txt', \n","                        transform=transform,\n","                        freq_threshold=5)\n","\n","class MyCollate:\n","    def __init__(self, pad_idx):\n","        self.pad_idx = pad_idx\n","        \n","        \n","    def __call__(self, batch):\n","        images = [item[0].unsqueeze(0) for item in batch]\n","        images = torch.cat(images, dim=0)\n","        \n","        # Pad the sequences with zeros to make them the same length\n","        lengths = [len(sample[1]) for sample in batch]\n","        max_length = max(lengths)\n","        padded_batch = torch.full((len(batch), max_length), self.pad_idx, dtype=torch.long)\n","        for i, sample in enumerate(batch):\n","            padded_batch[i, :len(sample[1])] = torch.LongTensor(sample[1])        \n","\n","        targets = padded_batch\n","        \n","        return images, targets    \n","    \n","    \n","pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n","\n","loader = DataLoader(\n","        dataset=dataset,\n","        batch_size=64,\n","        shuffle=True,\n","        collate_fn=MyCollate(pad_idx=pad_idx))\n","\n","\n","# x, y = next(iter(loader))\n","\n"]}]}